{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":297012053,"sourceType":"kernelVersion"}],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nYOLO Wrapper Class - Drop-in replacement for Ultralytics YOLO\nProvides the same interface for training, validation, and inference\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport cv2\nimport numpy as np\nimport yaml\nimport os\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport sys\n\nsys.path.append('/kaggle/usr/lib/notebooks/a21101131/yolov11-model/notebooks/a21101131')\n\nfrom yolov11_model import YOLOv11Model, make_anchors\n\nimport torchvision\n\n\n# ============================================================================\n# Loss Functions\n# ============================================================================\n\nclass YOLOLoss(nn.Module):\n    \"\"\"YOLOv11 Loss Function with focal loss, CIoU, and DFL.\"\"\"\n\n    def __init__(self, model, nc=80):\n        super().__init__()\n        self.nc = nc\n        self.reg_max = 16\n        self.topk = 10  # top-k candidates per GT box\n\n        # Loss weights\n        self.box_weight = 7.5\n        self.cls_weight = 0.5\n        self.dfl_weight = 1.5\n\n        # Focal loss parameters\n        self.focal_gamma = 1.5\n        self.focal_alpha = 0.25\n\n        # Get strides from model head\n        self.stride = model.head.stride  # e.g. [8, 16, 32]\n\n        # Cache DFL module (frozen weights, no need to recreate each step)\n        from yolov11_model import DFL as _DFL\n        self.dfl = _DFL(self.reg_max)\n\n    # ------------------------------------------------------------------\n    # Focal loss (replaces plain BCE)\n    # ------------------------------------------------------------------\n    @staticmethod\n    def focal_loss(pred, target, gamma=1.5, alpha=0.25, eps=1e-7):\n        \"\"\"Binary focal loss — downweights easy negatives so positives matter.\n        pred:   [*, nc] raw logits\n        target: [*, nc] soft targets in [0, 1]\n        Returns: scalar mean loss\n        \"\"\"\n        p = pred.sigmoid()\n        bce = nn.functional.binary_cross_entropy_with_logits(pred, target, reduction='none')\n        # Modulating factor: emphasises hard examples\n        p_t = p * target + (1 - p) * (1 - target)\n        modulating = (1 - p_t) ** gamma\n        # Alpha weighting\n        alpha_t = alpha * target + (1 - alpha) * (1 - target)\n        loss = alpha_t * modulating * bce\n        return loss\n\n    # ------------------------------------------------------------------\n    # Element-wise IoU (not pairwise)\n    # ------------------------------------------------------------------\n    @staticmethod\n    def elementwise_ciou(box1, box2, eps=1e-7):\n        \"\"\"CIoU between matched pairs. box1, box2: [N, 4] xyxy. Returns [N].\"\"\"\n        inter_x1 = torch.max(box1[:, 0], box2[:, 0])\n        inter_y1 = torch.max(box1[:, 1], box2[:, 1])\n        inter_x2 = torch.min(box1[:, 2], box2[:, 2])\n        inter_y2 = torch.min(box1[:, 3], box2[:, 3])\n        inter = (inter_x2 - inter_x1).clamp(0) * (inter_y2 - inter_y1).clamp(0)\n\n        a1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n        a2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n        union = a1 + a2 - inter + eps\n        iou = inter / union\n\n        # Enclosing box\n        enc_x1 = torch.min(box1[:, 0], box2[:, 0])\n        enc_y1 = torch.min(box1[:, 1], box2[:, 1])\n        enc_x2 = torch.max(box1[:, 2], box2[:, 2])\n        enc_y2 = torch.max(box1[:, 3], box2[:, 3])\n        c2 = (enc_x2 - enc_x1) ** 2 + (enc_y2 - enc_y1) ** 2 + eps\n        # Center distance\n        cx1, cy1 = (box1[:, 0] + box1[:, 2]) / 2, (box1[:, 1] + box1[:, 3]) / 2\n        cx2, cy2 = (box2[:, 0] + box2[:, 2]) / 2, (box2[:, 1] + box2[:, 3]) / 2\n        rho2 = (cx1 - cx2) ** 2 + (cy1 - cy2) ** 2\n        # Aspect ratio\n        w1, h1 = box1[:, 2] - box1[:, 0], box1[:, 3] - box1[:, 1]\n        w2, h2 = box2[:, 2] - box2[:, 0], box2[:, 3] - box2[:, 1]\n        v = (4 / (torch.pi ** 2)) * (torch.atan(w2 / (h2 + eps)) - torch.atan(w1 / (h1 + eps))) ** 2\n        alpha = v / (1 - iou + v + eps)\n        return iou - (rho2 / c2 + alpha * v)\n\n    @staticmethod\n    def elementwise_iou(box1, box2, eps=1e-7):\n        \"\"\"Plain IoU between matched pairs. box1, box2: [N, 4] xyxy. Returns [N].\"\"\"\n        inter_x1 = torch.max(box1[:, 0], box2[:, 0])\n        inter_y1 = torch.max(box1[:, 1], box2[:, 1])\n        inter_x2 = torch.min(box1[:, 2], box2[:, 2])\n        inter_y2 = torch.min(box1[:, 3], box2[:, 3])\n        inter = (inter_x2 - inter_x1).clamp(0) * (inter_y2 - inter_y1).clamp(0)\n        a1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n        a2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n        return inter / (a1 + a2 - inter + eps)\n\n    @staticmethod\n    def _xywh2xyxy(boxes):\n        \"\"\"Convert cx,cy,w,h to x1,y1,x2,y2.\"\"\"\n        cx, cy, w, h = boxes.unbind(-1)\n        return torch.stack([cx - w / 2, cy - h / 2, cx + w / 2, cy + h / 2], dim=-1)\n\n    # ------------------------------------------------------------------\n    # Target assignment (simplified center-based)\n    # ------------------------------------------------------------------\n    def assign_targets(self, anchors, strides, targets_list, pred_boxes, pred_cls, bs):\n        \"\"\"Assign GT boxes to anchors using a simplified center-prior strategy.\"\"\"\n        device = anchors.device\n        N = anchors.shape[0]\n        anchor_pixels = anchors * strides  # [N, 2] anchor center in pixels (x, y)\n\n        assigned_gt_boxes = torch.zeros(bs, N, 4, device=device)\n        assigned_cls = torch.zeros(bs, N, self.nc, device=device)\n        fg_mask = torch.zeros(bs, N, dtype=torch.bool, device=device)\n        assigned_ltrb = torch.zeros(bs, N, 4, device=device)\n\n        for b in range(bs):\n            targets = targets_list[b]  # [M, 5]\n            if len(targets) == 0:\n                continue\n            targets = targets.to(device)\n            gt_cls = targets[:, 0].long()  # [M]\n            gt_cxcywh = targets[:, 1:5]    # [M, 4] normalised\n            # Scale GT to 640 pixel space\n            gt_cxcywh_px = gt_cxcywh * 640  # [M, 4]\n            gt_xyxy = self._xywh2xyxy(gt_cxcywh_px)  # [M, 4]\n\n            M = gt_xyxy.shape[0]\n            ax = anchor_pixels[:, 0]  # [N]\n            ay = anchor_pixels[:, 1]  # [N]\n\n            # [M, N] mask: anchor center inside GT box\n            inside_x = (ax.unsqueeze(0) >= gt_xyxy[:, 0:1]) & (ax.unsqueeze(0) <= gt_xyxy[:, 2:3])\n            inside_y = (ay.unsqueeze(0) >= gt_xyxy[:, 1:2]) & (ay.unsqueeze(0) <= gt_xyxy[:, 3:4])\n            inside = inside_x & inside_y  # [M, N]\n\n            pred_b_xyxy = self._xywh2xyxy(pred_boxes[b])  # [N, 4]\n\n            for j in range(M):\n                cand_mask = inside[j]  # [N]\n                if cand_mask.sum() == 0:\n                    gt_cx, gt_cy = gt_cxcywh_px[j, 0], gt_cxcywh_px[j, 1]\n                    dists = (ax - gt_cx) ** 2 + (ay - gt_cy) ** 2\n                    _, topk_idx = dists.topk(min(self.topk, N), largest=False)\n                    cand_mask = torch.zeros(N, dtype=torch.bool, device=device)\n                    cand_mask[topk_idx] = True\n\n                cand_idx = cand_mask.nonzero(as_tuple=False).squeeze(-1)\n                if cand_idx.dim() == 0:\n                    cand_idx = cand_idx.unsqueeze(0)\n\n                # IoU between candidates' predicted boxes and this GT\n                gt_j_expanded = gt_xyxy[j:j+1].expand(cand_idx.shape[0], 4)\n                iou = self.elementwise_iou(pred_b_xyxy[cand_idx], gt_j_expanded)  # [K]\n\n                # Pick top-k by IoU\n                k = min(self.topk, iou.shape[0])\n                _, topk_local = iou.topk(k)\n                sel_idx = cand_idx[topk_local]\n\n                fg_mask[b, sel_idx] = True\n                assigned_gt_boxes[b, sel_idx] = gt_xyxy[j]\n\n                # Class target: hard label = 1.0 (not IoU-weighted, so positives get full signal)\n                cls_target = torch.zeros(self.nc, device=device)\n                cls_target[gt_cls[j]] = 1.0\n                assigned_cls[b, sel_idx] = cls_target.unsqueeze(0)\n\n                # LTRB in grid units for DFL\n                anc_sel = anchor_pixels[sel_idx]  # [k, 2]\n                ltrb_l = anc_sel[:, 0] - gt_xyxy[j, 0]\n                ltrb_t = anc_sel[:, 1] - gt_xyxy[j, 1]\n                ltrb_r = gt_xyxy[j, 2] - anc_sel[:, 0]\n                ltrb_b = gt_xyxy[j, 3] - anc_sel[:, 1]\n                ltrb = torch.stack([ltrb_l, ltrb_t, ltrb_r, ltrb_b], dim=-1)  # [k, 4]\n                ltrb = ltrb / strides[sel_idx]  # convert to grid units\n                assigned_ltrb[b, sel_idx] = ltrb\n\n        return assigned_gt_boxes, assigned_cls, fg_mask, assigned_ltrb\n\n    # ------------------------------------------------------------------\n    # Forward\n    # ------------------------------------------------------------------\n    def forward(self, preds, targets):\n        \"\"\"\n        Args:\n            preds: list of feature maps [P3, P4, P5], each [B, no, H, W]\n            targets: tuple of label tensors per image, each [M, 5] (cls, cx, cy, w, h) normalised\n        Returns:\n            loss: scalar\n            loss_items: [lbox, lcls, ldfl] detached\n        \"\"\"\n        device = preds[0].device\n        bs = preds[0].shape[0]\n        reg_max = self.reg_max\n\n        # Build anchors from feature maps\n        anchors, strides = make_anchors(preds, self.stride.to(device))\n        N = anchors.shape[0]\n\n        # Concatenate predictions across levels: [B, no, N]\n        no = reg_max * 4 + self.nc\n        pred_cat = torch.cat([p.view(bs, no, -1) for p in preds], dim=2)\n        pred_box_raw = pred_cat[:, :reg_max * 4, :]  # [B, 64, N]\n        pred_cls_raw = pred_cat[:, reg_max * 4:, :]   # [B, nc, N]\n\n        # DFL decode to get LTRB\n        self.dfl = self.dfl.to(device)\n        pred_ltrb = self.dfl(pred_box_raw)  # [B, 4, N]\n\n        # Decode LTRB to cxcywh in pixels\n        anc_t = anchors.transpose(0, 1).unsqueeze(0)  # [1, 2, N]\n        str_t = strides.transpose(0, 1).unsqueeze(0)  # [1, 1, N]\n        lt, rb = pred_ltrb.chunk(2, dim=1)\n        x1y1 = anc_t - lt\n        x2y2 = anc_t + rb\n        cxcy = (x1y1 + x2y2) / 2\n        wh = (x2y2 - x1y1).clamp(min=0)\n        pred_boxes_px = torch.cat([cxcy, wh], dim=1) * str_t  # [B, 4, N]\n\n        pred_boxes_t = pred_boxes_px.permute(0, 2, 1)  # [B, N, 4]\n        pred_cls_t = pred_cls_raw.permute(0, 2, 1)      # [B, N, nc]\n\n        # Assign targets (detach predictions so assignment doesn't affect gradients)\n        with torch.no_grad():\n            assigned_gt, assigned_cls, fg_mask, assigned_ltrb = self.assign_targets(\n                anchors, strides, targets, pred_boxes_t.detach(), pred_cls_t.detach(), bs\n            )\n\n        num_pos = fg_mask.sum().clamp(min=1).float()\n\n        # --- Box loss (CIoU on positive anchors, element-wise) ---\n        lbox = torch.zeros(1, device=device)\n        if fg_mask.any():\n            pred_pos_xyxy = self._xywh2xyxy(pred_boxes_t[fg_mask])  # [P, 4]\n            gt_pos_xyxy = assigned_gt[fg_mask]                        # [P, 4]\n            ciou = self.elementwise_ciou(pred_pos_xyxy, gt_pos_xyxy)  # [P]\n            lbox = (1.0 - ciou).mean()\n\n        # --- Classification loss (FOCAL LOSS on all anchors) ---\n        # This downweights the ~8370 easy negatives so positives actually matter\n        cls_loss_all = self.focal_loss(\n            pred_cls_t, assigned_cls,\n            gamma=self.focal_gamma, alpha=self.focal_alpha\n        )  # [B, N, nc]\n        lcls = cls_loss_all.sum() / num_pos\n\n        # --- DFL loss (cross-entropy on raw 16-bin distributions for positives) ---\n        ldfl = torch.zeros(1, device=device)\n        if fg_mask.any():\n            raw_box = pred_box_raw.permute(0, 2, 1)  # [B, N, 64]\n            raw_pos = raw_box[fg_mask]                 # [P, 64]\n            target_ltrb = assigned_ltrb[fg_mask]       # [P, 4]\n            target_ltrb = target_ltrb.clamp(0, reg_max - 1 - 0.01)\n            raw_pos = raw_pos.view(-1, 4, reg_max)     # [P, 4, 16]\n            tl = target_ltrb.long()\n            tr = (tl + 1).clamp(max=reg_max - 1)\n            wl = tr.float() - target_ltrb\n            wr = 1.0 - wl\n            log_probs = nn.functional.log_softmax(raw_pos, dim=-1)\n            loss_l = -log_probs.gather(-1, tl.unsqueeze(-1)).squeeze(-1) * wl\n            loss_r = -log_probs.gather(-1, tr.unsqueeze(-1)).squeeze(-1) * wr\n            ldfl = (loss_l + loss_r).mean()\n\n        loss = self.box_weight * lbox + self.cls_weight * lcls + self.dfl_weight * ldfl\n        return loss, torch.cat([lbox.reshape(1), lcls.reshape(1), ldfl.reshape(1)]).detach()\n\n\n# ============================================================================\n# Dataset\n# ============================================================================\n\nclass YOLODataset(Dataset):\n    \"\"\"YOLO Dataset for loading images and labels\"\"\"\n    def __init__(self, img_dir, label_dir, img_size=640, augment=False):\n        self.img_dir = Path(img_dir)\n        self.label_dir = Path(label_dir)\n        self.img_size = img_size\n        self.augment = augment\n        \n        # Get all image files\n        self.img_files = sorted(list(self.img_dir.glob('*.jpg')) + \n                               list(self.img_dir.glob('*.png')))\n        \n        print(f\"Found {len(self.img_files)} images in {img_dir}\")\n\n    def __len__(self):\n        return len(self.img_files)\n\n    def __getitem__(self, idx):\n        # Load image\n        img_path = self.img_files[idx]\n        img = cv2.imread(str(img_path))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # Load labels\n        label_path = self.label_dir / (img_path.stem + '.txt')\n        labels = []\n        if label_path.exists():\n            with open(label_path, 'r') as f:\n                for line in f:\n                    cls, x, y, w, h = map(float, line.strip().split())\n                    labels.append([cls, x, y, w, h])\n        \n        labels = np.array(labels) if labels else np.zeros((0, 5))\n        \n        # Resize image\n        img, labels = self.resize_image(img, labels)\n        \n        # Convert to tensor\n        img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n        labels = torch.from_numpy(labels).float()\n        \n        return img, labels\n\n    def resize_image(self, img, labels):\n        \"\"\"Resize image to target size while maintaining aspect ratio\"\"\"\n        h, w = img.shape[:2]\n        scale = min(self.img_size / h, self.img_size / w)\n        new_h, new_w = int(h * scale), int(w * scale)\n\n        # Resize\n        img = cv2.resize(img, (new_w, new_h))\n\n        # Pad to square\n        pad_h = self.img_size - new_h\n        pad_w = self.img_size - new_w\n        top = pad_h // 2\n        left = pad_w // 2\n\n        img_padded = np.full((self.img_size, self.img_size, 3), 114, dtype=np.uint8)\n        img_padded[top:top+new_h, left:left+new_w] = img\n\n        # Adjust labels for letterbox: original normalised coords -> 640-space normalised coords\n        if len(labels) > 0:\n            # labels[:, 1:] are (cx, cy, w, h) normalised to original image\n            # Convert to pixel coords in original image, then to padded image, then re-normalise\n            labels = labels.copy()\n            # cx, cy in pixels of resized (not padded) image\n            labels[:, 1] = labels[:, 1] * new_w + left   # cx in padded image (pixels)\n            labels[:, 2] = labels[:, 2] * new_h + top     # cy in padded image (pixels)\n            labels[:, 3] = labels[:, 3] * new_w            # w in padded image (pixels)\n            labels[:, 4] = labels[:, 4] * new_h            # h in padded image (pixels)\n            # Re-normalise to [0, 1] in padded 640x640 space\n            labels[:, 1] /= self.img_size\n            labels[:, 2] /= self.img_size\n            labels[:, 3] /= self.img_size\n            labels[:, 4] /= self.img_size\n\n        return img_padded, labels\n\n\n# ============================================================================\n# Metrics and Evaluation\n# ============================================================================\n\nclass Metrics:\n    \"\"\"Metrics for object detection\"\"\"\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.map50 = 0.0\n        self.map = 0.0\n        self.mp = 0.0  # mean precision\n        self.mr = 0.0  # mean recall\n        \n    def __str__(self):\n        return f\"mAP@0.5: {self.map50:.4f}, mAP@0.5:0.95: {self.map:.4f}, P: {self.mp:.4f}, R: {self.mr:.4f}\"\n\n\nclass BoxMetrics:\n    \"\"\"Box metrics wrapper\"\"\"\n    def __init__(self):\n        self.map50 = 0.0\n        self.map = 0.0\n        self.mp = 0.0\n        self.mr = 0.0\n\n\nclass ValidationResults:\n    \"\"\"Validation results container\"\"\"\n    def __init__(self):\n        self.box = BoxMetrics()\n\n\n# ============================================================================\n# Trainer\n# ============================================================================\n\nclass Trainer:\n    \"\"\"Training logic for YOLO model\"\"\"\n    def __init__(self, model, data_config, args):\n        self.model = model\n        self.data_config = data_config\n        self.args = args\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Move model to device\n        self.model.to(self.device)\n        \n        # Loss function\n        self.criterion = YOLOLoss(model, nc=self.data_config['nc'])\n        \n        # Optimizer\n        self.optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0005)\n        \n        # Learning rate scheduler\n        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(\n            self.optimizer, T_max=args['epochs'], eta_min=0.00001\n        )\n        \n        # Datasets\n        self.train_dataset = YOLODataset(\n            os.path.join(data_config['path'], data_config['train']),\n            os.path.join(data_config['path'], 'labels/train'),\n            img_size=args['imgsz']\n        )\n        \n        self.val_dataset = YOLODataset(\n            os.path.join(data_config['path'], data_config['val']),\n            os.path.join(data_config['path'], 'labels/val'),\n            img_size=args['imgsz']\n        )\n        \n        # Dataloaders\n        self.train_loader = DataLoader(\n            self.train_dataset,\n            batch_size=args['batch'],\n            shuffle=True,\n            num_workers=args['workers'],\n            pin_memory=True,\n            collate_fn=self.collate_fn\n        )\n        \n        self.val_loader = DataLoader(\n            self.val_dataset,\n            batch_size=args['batch'],\n            shuffle=False,\n            num_workers=args['workers'],\n            pin_memory=True,\n            collate_fn=self.collate_fn\n        )\n        \n        # Training state\n        self.epoch = 0\n        self.best_fitness = 0.0\n        \n        # Create save directory\n        self.save_dir = Path(args['project']) / args['name']\n        self.save_dir.mkdir(parents=True, exist_ok=True)\n        (self.save_dir / 'weights').mkdir(exist_ok=True)\n\n    def collate_fn(self, batch):\n        \"\"\"Custom collate function for batching\"\"\"\n        imgs, labels = zip(*batch)\n        imgs = torch.stack(imgs, 0)\n        return imgs, labels\n\n    def train(self):\n        \"\"\"Main training loop\"\"\"\n        print(f\"\\nStarting training for {self.args['epochs']} epochs...\")\n        print(f\"Device: {self.device}\")\n        print(f\"Training images: {len(self.train_dataset)}\")\n        print(f\"Validation images: {len(self.val_dataset)}\")\n        \n        for epoch in range(self.args['epochs']):\n            self.epoch = epoch\n            print(f\"\\nEpoch {epoch + 1}/{self.args['epochs']}\")\n            \n            # Train one epoch\n            self.train_one_epoch()\n            \n            # Validate\n            if (epoch + 1) % 5 == 0 or epoch == self.args['epochs'] - 1:\n                metrics = self.validate()\n                \n                # Save best model\n                fitness = metrics.map50  # Use mAP@0.5 as fitness\n                if fitness > self.best_fitness:\n                    self.best_fitness = fitness\n                    self.save_checkpoint('best.pt')\n                    print(f\"New best model saved! mAP@0.5: {fitness:.4f}\")\n            \n            # Save last\n            self.save_checkpoint('last.pt')\n            \n            # Update learning rate\n            self.scheduler.step()\n        \n        print(\"\\nTraining complete!\")\n        return {'success': True}\n\n    def train_one_epoch(self):\n        \"\"\"Train for one epoch\"\"\"\n        self.model.train()\n        pbar = tqdm(self.train_loader, desc='Training')\n        \n        total_loss = 0\n        for i, (imgs, targets) in enumerate(pbar):\n            imgs = imgs.to(self.device)\n            \n            # Forward\n            self.optimizer.zero_grad()\n            preds = self.model(imgs)\n            \n            # Calculate loss\n            loss, loss_items = self.criterion(preds, targets)\n            \n            # Backward\n            loss.backward()\n            self.optimizer.step()\n            \n            # Update progress bar\n            total_loss += loss.item()\n            pbar.set_postfix({'loss': f'{loss.item():.4f}', \n                            'avg_loss': f'{total_loss/(i+1):.4f}'})\n\n    def validate(self):\n        \"\"\"Validate the model by running inference and comparing to GT.\"\"\"\n        self.model.eval()\n        print(\"\\nValidating...\")\n\n        iou_threshold = 0.5\n        all_tp = 0       # true positives at IoU >= 0.5\n        all_fp = 0       # false positives\n        all_n_gt = 0     # total ground truth boxes\n        all_scores = []   # for precision-recall computation\n\n        nc = self.data_config['nc']\n\n        with torch.no_grad():\n            for imgs, targets in tqdm(self.val_loader, desc='Validation'):\n                imgs = imgs.to(self.device)\n                preds = self.model(imgs)  # inference mode: (y, x)\n                if isinstance(preds, tuple):\n                    preds = preds[0]  # [B, 4+nc, N]\n\n                bs = imgs.shape[0]\n                for b in range(bs):\n                    pred_b = preds[b]  # [4+nc, N]\n                    pred_b = pred_b.transpose(0, 1)  # [N, 4+nc]\n                    boxes = pred_b[:, :4]  # cx, cy, w, h in pixels\n                    scores = pred_b[:, 4:]  # [N, nc]\n                    max_scores, max_cls = scores.max(dim=1)\n\n                    # Confidence filter\n                    keep = max_scores >= 0.25\n                    if keep.sum() > 0:\n                        boxes = boxes[keep]\n                        max_scores = max_scores[keep]\n                        max_cls = max_cls[keep]\n                        # cx,cy,w,h -> xyxy\n                        x1 = boxes[:, 0] - boxes[:, 2] / 2\n                        y1 = boxes[:, 1] - boxes[:, 3] / 2\n                        x2 = boxes[:, 0] + boxes[:, 2] / 2\n                        y2 = boxes[:, 1] + boxes[:, 3] / 2\n                        det_xyxy = torch.stack([x1, y1, x2, y2], dim=1)\n                        # NMS\n                        nms_keep = torchvision.ops.nms(det_xyxy, max_scores, 0.45)\n                        det_xyxy = det_xyxy[nms_keep]\n                        det_scores = max_scores[nms_keep]\n                    else:\n                        det_xyxy = torch.zeros(0, 4, device=imgs.device)\n                        det_scores = torch.zeros(0, device=imgs.device)\n\n                    # Ground truth for this image (normalised -> 640 pixels)\n                    gt = targets[b]  # [M, 5]\n                    if len(gt) > 0:\n                        gt = gt.to(imgs.device)\n                        gt_xyxy = torch.stack([\n                            (gt[:, 1] - gt[:, 3] / 2) * 640,\n                            (gt[:, 2] - gt[:, 4] / 2) * 640,\n                            (gt[:, 1] + gt[:, 3] / 2) * 640,\n                            (gt[:, 2] + gt[:, 4] / 2) * 640,\n                        ], dim=1)  # [M, 4]\n                        n_gt = gt_xyxy.shape[0]\n                    else:\n                        gt_xyxy = torch.zeros(0, 4, device=imgs.device)\n                        n_gt = 0\n\n                    all_n_gt += n_gt\n                    n_det = det_xyxy.shape[0]\n\n                    if n_det == 0:\n                        continue\n                    if n_gt == 0:\n                        all_fp += n_det\n                        continue\n\n                    # Compute IoU between detections and GTs\n                    # det_xyxy: [D, 4], gt_xyxy: [G, 4]\n                    d = det_xyxy.unsqueeze(1)  # [D, 1, 4]\n                    g = gt_xyxy.unsqueeze(0)    # [1, G, 4]\n                    ix1 = torch.max(d[..., 0], g[..., 0])\n                    iy1 = torch.max(d[..., 1], g[..., 1])\n                    ix2 = torch.min(d[..., 2], g[..., 2])\n                    iy2 = torch.min(d[..., 3], g[..., 3])\n                    inter = (ix2 - ix1).clamp(0) * (iy2 - iy1).clamp(0)\n                    a_d = (d[..., 2] - d[..., 0]) * (d[..., 3] - d[..., 1])\n                    a_g = (g[..., 2] - g[..., 0]) * (g[..., 3] - g[..., 1])\n                    iou_mat = inter / (a_d + a_g - inter + 1e-7)  # [D, G]\n\n                    # Greedy matching: for each detection (sorted by score), match to best GT\n                    matched_gt = set()\n                    sorted_idx = det_scores.argsort(descending=True)\n                    for di in sorted_idx:\n                        ious = iou_mat[di]  # [G]\n                        best_gt = ious.argmax().item()\n                        if ious[best_gt] >= iou_threshold and best_gt not in matched_gt:\n                            all_tp += 1\n                            matched_gt.add(best_gt)\n                        else:\n                            all_fp += 1\n\n        # Compute metrics\n        metrics = ValidationResults()\n        precision = all_tp / (all_tp + all_fp + 1e-7)\n        recall = all_tp / (all_n_gt + 1e-7)\n        # Approximate mAP@0.5 as F1-like measure (proper AP requires full PR curve)\n        metrics.box.map50 = 2 * precision * recall / (precision + recall + 1e-7)\n        metrics.box.map = metrics.box.map50 * 0.6  # rough estimate for 0.5:0.95\n        metrics.box.mp = precision\n        metrics.box.mr = recall\n\n        print(f\"\\nValidation Results:\")\n        print(f\"  mAP@0.5: {metrics.box.map50:.4f}\")\n        print(f\"  mAP@0.5:0.95: {metrics.box.map:.4f}\")\n        print(f\"  Precision: {metrics.box.mp:.4f}\")\n        print(f\"  Recall: {metrics.box.mr:.4f}\")\n        print(f\"  (TP={all_tp}, FP={all_fp}, GT={all_n_gt})\")\n\n        return metrics.box\n\n    def save_checkpoint(self, filename):\n        \"\"\"Save model checkpoint\"\"\"\n        checkpoint = {\n            'epoch': self.epoch,\n            'model': self.model.state_dict(),\n            'optimizer': self.optimizer.state_dict(),\n            'best_fitness': self.best_fitness,\n            'nc': self.model.nc,\n        }\n        torch.save(checkpoint, self.save_dir / 'weights' / filename)\n\n\n# ============================================================================\n# Inference and Results\n# ============================================================================\n\nclass Results:\n    \"\"\"Results class for inference\"\"\"\n    def __init__(self, orig_img, boxes, scores, classes, names):\n        self.orig_img = orig_img\n        self.boxes = boxes\n        self.scores = scores\n        self.classes = classes\n        self.names = names\n\n    def plot(self):\n        \"\"\"Plot results on image\"\"\"\n        img = self.orig_img.copy()\n        \n        for box, score, cls in zip(self.boxes, self.scores, self.classes):\n            x1, y1, x2, y2 = map(int, box)\n            \n            # Draw box\n            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            \n            # Draw label\n            label = f'{self.names[int(cls)]} {score:.2f}'\n            cv2.putText(img, label, (x1, y1 - 10), \n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n        \n        return img\n\n\n# ============================================================================\n# Main YOLO Class\n# ============================================================================\n\nclass YOLO:\n    \"\"\"\n    YOLOv11 Interface - Drop-in replacement for Ultralytics YOLO\n    \n    Usage:\n        model = YOLO('yolo11n.pt')  # Load pretrained\n        results = model.train(data='data.yaml', epochs=100)\n        metrics = model.val()\n        results = model.predict('image.jpg')\n    \"\"\"\n    \n    def __init__(self, model='yolo11n.pt', task='detect'):\n        \"\"\"\n        Initialize YOLO model\n        \n        Args:\n            model: Model size ('yolo11n.pt', 'yolo11s.pt', etc.) or path to weights\n            task: Task type ('detect', 'segment', 'classify')\n        \"\"\"\n        self.task = task\n        self.model_path = model\n        \n        # Extract model size from filename\n        if 'yolo11' in str(model).lower():\n            size = str(model).lower().replace('yolo11', '').replace('.pt', '').replace('yolov11', '')\n            if not size or size not in ['n', 's', 'm', 'l', 'x']:\n                size = 'n'  # default to nano\n        else:\n            size = 'n'\n        \n        self.model_size = size\n        self.model = None\n        self.trainer = None\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Load model if weights exist\n        if os.path.exists(model):\n            self.load(model)\n        else:\n            print(f\"Initializing new YOLOv11{size.upper()} model...\")\n            self.model = YOLOv11Model(nc=80, model_size=size)\n\n    def load(self, weights):\n        \"\"\"Load model weights\"\"\"\n        print(f\"Loading weights from {weights}...\")\n        \n        # Try to load checkpoint\n        if os.path.exists(weights):\n            checkpoint = torch.load(weights, map_location='cpu')\n            \n            if isinstance(checkpoint, dict) and 'model' in checkpoint:\n                # Load from our checkpoint format\n                nc = checkpoint.get('nc', None)\n                if nc is None:\n                    # Infer nc from saved cv3 final conv output shape\n                    nc = checkpoint['model']['head.cv3.0.2.weight'].shape[0]\n                self.model = YOLOv11Model(nc=nc, model_size=self.model_size)\n                self.model.load_state_dict(checkpoint['model'])\n            else:\n                # Initialize new model (can't load Ultralytics weights directly)\n                print(\"Note: Creating new model (cannot load Ultralytics weights)\")\n                self.model = YOLOv11Model(nc=80, model_size=self.model_size)\n        else:\n            self.model = YOLOv11Model(nc=80, model_size=self.model_size)\n        \n        self.model.to(self.device)\n        print(\"Model loaded successfully!\")\n\n    def train(self, data, epochs=100, imgsz=640, batch=16, device=0, \n              workers=2, project='runs/detect', name='exp', patience=50,\n              save=True, plots=True, **kwargs):\n        \"\"\"\n        Train the model\n        \n        Args:\n            data: Path to data.yaml configuration file\n            epochs: Number of training epochs\n            imgsz: Input image size\n            batch: Batch size\n            device: Device to train on\n            workers: Number of dataloader workers\n            project: Project name\n            name: Experiment name\n            patience: Early stopping patience\n            save: Save checkpoints\n            plots: Create training plots\n        \"\"\"\n        # Load data config\n        with open(data, 'r') as f:\n            data_config = yaml.safe_load(f)\n        \n        # Initialize model with correct number of classes\n        nc = data_config['nc']\n        if self.model is None or self.model.nc != nc:\n            self.model = YOLOv11Model(nc=nc, model_size=self.model_size)\n        \n        # Training arguments\n        args = {\n            'epochs': epochs,\n            'imgsz': imgsz,\n            'batch': batch,\n            'device': device,\n            'workers': workers,\n            'project': project,\n            'name': name,\n            'patience': patience,\n            'save': save,\n            'plots': plots,\n        }\n        \n        # Create trainer\n        self.trainer = Trainer(self.model, data_config, args)\n        \n        # Train\n        results = self.trainer.train()\n        \n        return results\n\n    def val(self, data=None, **kwargs):\n        \"\"\"Validate the model\"\"\"\n        if self.trainer is None:\n            print(\"No trainer available. Please train the model first or provide data config.\")\n            metrics = ValidationResults()\n            return metrics.box\n\n        return self.trainer.validate()\n\n    def predict(self, source, save=False, conf=0.25, iou=0.45, **kwargs):\n        \"\"\"\n        Run inference on images\n\n        Args:\n            source: Image path or directory\n            save: Save results\n            conf: Confidence threshold\n            iou: IoU threshold for NMS\n        \"\"\"\n        self.model.eval()\n        self.model.to(self.device)\n\n        # Build class names\n        nc = self.model.nc\n        names = {i: str(i) for i in range(nc)}\n        if nc == 1:\n            names = {0: 'face'}\n\n        # Handle single image or directory\n        if isinstance(source, str):\n            if os.path.isfile(source):\n                image_paths = [source]\n            elif os.path.isdir(source):\n                image_paths = sorted(\n                    list(Path(source).glob('*.jpg')) + list(Path(source).glob('*.png'))\n                )\n            else:\n                raise ValueError(f\"Invalid source: {source}\")\n        else:\n            image_paths = [source]\n\n        results = []\n\n        for img_path in image_paths:\n            # Load original image\n            img = cv2.imread(str(img_path))\n            orig_h, orig_w = img.shape[:2]\n            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n            # Letterbox resize to 640x640 (same as dataset)\n            imgsz = 640\n            scale = min(imgsz / orig_h, imgsz / orig_w)\n            new_h, new_w = int(orig_h * scale), int(orig_w * scale)\n            img_resized = cv2.resize(img_rgb, (new_w, new_h))\n\n            pad_h = imgsz - new_h\n            pad_w = imgsz - new_w\n            top = pad_h // 2\n            left = pad_w // 2\n            img_padded = np.full((imgsz, imgsz, 3), 114, dtype=np.uint8)\n            img_padded[top:top + new_h, left:left + new_w] = img_resized\n\n            # To tensor\n            img_tensor = torch.from_numpy(img_padded).permute(2, 0, 1).float() / 255.0\n            img_tensor = img_tensor.unsqueeze(0).to(self.device)\n\n            # Inference — returns (y, x) where y is [B, 4+nc, N]\n            with torch.no_grad():\n                pred = self.model(img_tensor)\n\n            if isinstance(pred, tuple):\n                pred = pred[0]  # take decoded output\n\n            # pred: [1, 4+nc, N] -> [N, 4+nc]\n            pred = pred[0].transpose(0, 1)  # [N, 4+nc]\n\n            # Split box (cx, cy, w, h) and class scores\n            box_cxcywh = pred[:, :4]\n            cls_scores = pred[:, 4:]  # [N, nc]\n\n            # Get max class score per anchor\n            max_scores, max_cls = cls_scores.max(dim=1)  # [N]\n\n            # Confidence filter\n            keep = max_scores >= conf\n            if keep.sum() == 0:\n                results.append(Results(img, np.zeros((0, 4)), np.array([]), np.array([]), names))\n                continue\n\n            box_cxcywh = box_cxcywh[keep]\n            max_scores = max_scores[keep]\n            max_cls = max_cls[keep]\n\n            # Convert cx, cy, w, h -> x1, y1, x2, y2 (in 640x640 space)\n            cx, cy, w, h = box_cxcywh[:, 0], box_cxcywh[:, 1], box_cxcywh[:, 2], box_cxcywh[:, 3]\n            x1 = cx - w / 2\n            y1 = cy - h / 2\n            x2 = cx + w / 2\n            y2 = cy + h / 2\n            boxes_xyxy = torch.stack([x1, y1, x2, y2], dim=1)  # [K, 4]\n\n            # NMS (class-agnostic for simplicity with single class; for multi-class, offset by class)\n            nms_keep = torchvision.ops.nms(boxes_xyxy, max_scores, iou)\n            boxes_xyxy = boxes_xyxy[nms_keep]\n            max_scores = max_scores[nms_keep]\n            max_cls = max_cls[nms_keep]\n\n            # Scale boxes from 640x640 letterbox back to original image\n            boxes_xyxy[:, 0] = (boxes_xyxy[:, 0] - left) / scale\n            boxes_xyxy[:, 1] = (boxes_xyxy[:, 1] - top) / scale\n            boxes_xyxy[:, 2] = (boxes_xyxy[:, 2] - left) / scale\n            boxes_xyxy[:, 3] = (boxes_xyxy[:, 3] - top) / scale\n\n            # Clamp to image bounds\n            boxes_xyxy[:, 0].clamp_(0, orig_w)\n            boxes_xyxy[:, 1].clamp_(0, orig_h)\n            boxes_xyxy[:, 2].clamp_(0, orig_w)\n            boxes_xyxy[:, 3].clamp_(0, orig_h)\n\n            result = Results(\n                img,\n                boxes_xyxy.cpu().numpy(),\n                max_scores.cpu().numpy(),\n                max_cls.cpu().numpy(),\n                names,\n            )\n            results.append(result)\n\n        return results\n\n    def export(self, format='onnx', **kwargs):\n        \"\"\"Export model to different formats\"\"\"\n        print(f\"Exporting model to {format}...\")\n        \n        if format == 'onnx':\n            dummy_input = torch.randn(1, 3, 640, 640).to(self.device)\n            torch.onnx.export(\n                self.model,\n                dummy_input,\n                f\"yolov11{self.model_size}.onnx\",\n                input_names=['images'],\n                output_names=['output'],\n                dynamic_axes={'images': {0: 'batch'}, 'output': {0: 'batch'}}\n            )\n            print(f\"Model exported to yolov11{self.model_size}.onnx\")\n        else:\n            print(f\"Export format {format} not implemented yet\")\n\n\n# Example usage\nprint(\"YOLOv11 Custom Implementation\")\nprint(\"=\" * 50)\n\n# Initialize model\nmodel = YOLO('yolo11n.pt')\nprint(f\"Model initialized: YOLOv11{model.model_size.upper()}\")\nprint(f\"Device: {model.device}\")\nprint(f\"Parameters: {sum(p.numel() for p in model.model.parameters()):,}\")\n","metadata":{"_uuid":"5529cb86-5285-4b80-b5fc-df5be2f12718","_cell_guid":"79955e0c-b8ff-40ad-aea0-ddd5afa91b9e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-11T11:26:57.034519Z","iopub.execute_input":"2026-02-11T11:26:57.035566Z","iopub.status.idle":"2026-02-11T11:26:57.233605Z","shell.execute_reply.started":"2026-02-11T11:26:57.035523Z","shell.execute_reply":"2026-02-11T11:26:57.232737Z"}},"outputs":[{"name":"stdout","text":"YOLOv11 Custom Implementation\n==================================================\nInitializing new YOLOv11N model...\nModel initialized: YOLOv11N\nDevice: cpu\nParameters: 2,656,000\n","output_type":"stream"}],"execution_count":3}]}