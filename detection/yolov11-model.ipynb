{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nCustom YOLOv11 Implementation from Scratch\nThis implementation provides a drop-in replacement for Ultralytics YOLO\nwith the same interface for training and inference.\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import List, Tuple, Optional, Dict, Union\nimport math\nimport yaml\nimport os\nfrom pathlib import Path\n\n\n# ============================================================================\n# Core Building Blocks\n# ============================================================================\n\nclass Conv(nn.Module):\n    \"\"\"Standard convolution with BatchNorm and SiLU activation\"\"\"\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):\n        super().__init__()\n        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n        self.bn = nn.BatchNorm2d(c2)\n        self.act = nn.SiLU() if act else nn.Identity()\n\n    def forward(self, x):\n        return self.act(self.bn(self.conv(x)))\n\n\nclass DWConv(Conv):\n    \"\"\"Depthwise convolution\"\"\"\n    def __init__(self, c1, c2, k=1, s=1, p=None, act=True):\n        super().__init__(c1, c2, k, s, p, g=math.gcd(c1, c2), act=act)\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"Standard bottleneck block with residual connection\"\"\"\n    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, k[0], 1)\n        self.cv2 = Conv(c_, c2, k[1], 1, g=g)\n        self.add = shortcut and c1 == c2\n\n    def forward(self, x):\n        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n\n\nclass C2f(nn.Module):\n    \"\"\"CSP Bottleneck with 2 convolutions - YOLOv8/v11 style\"\"\"\n    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):\n        super().__init__()\n        self.c = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n        self.cv2 = Conv((2 + n) * self.c, c2, 1)\n        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))\n\n    def forward(self, x):\n        y = list(self.cv1(x).chunk(2, 1))\n        y.extend(m(y[-1]) for m in self.m)\n        return self.cv2(torch.cat(y, 1))\n\n\nclass SPPF(nn.Module):\n    \"\"\"Spatial Pyramid Pooling - Fast (SPPF) layer\"\"\"\n    def __init__(self, c1, c2, k=5):\n        super().__init__()\n        c_ = c1 // 2  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_ * 4, c2, 1, 1)\n        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n\n    def forward(self, x):\n        x = self.cv1(x)\n        y1 = self.m(x)\n        y2 = self.m(y1)\n        return self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1))\n\n\nclass C2fCIB(nn.Module):\n    \"\"\"C2f with Contextual Information Block - YOLOv11 specific\"\"\"\n    def __init__(self, c1, c2, n=1, shortcut=False, lk=False, g=1, e=0.5):\n        super().__init__()\n        self.c = int(c2 * e)\n        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n        self.cv2 = Conv((2 + n) * self.c, c2, 1)\n        self.m = nn.ModuleList(\n            CIB(self.c, self.c, shortcut, e=1.0, lk=lk) for _ in range(n)\n        )\n\n    def forward(self, x):\n        y = list(self.cv1(x).chunk(2, 1))\n        y.extend(m(y[-1]) for m in self.m)\n        return self.cv2(torch.cat(y, 1))\n\n\nclass CIB(nn.Module):\n    \"\"\"Contextual Information Block\"\"\"\n    def __init__(self, c1, c2, shortcut=True, e=0.5, lk=False):\n        super().__init__()\n        c_ = int(c2 * e)\n        self.cv1 = nn.Sequential(\n            Conv(c1, c1, 3, g=c1),\n            Conv(c1, 2 * c_, 1),\n            Conv(2 * c_, 2 * c_, 3, g=2 * c_) if not lk else Conv(2 * c_, 2 * c_, 9, g=2 * c_),\n            Conv(2 * c_, c2, 1),\n        )\n        self.add = shortcut and c1 == c2\n\n    def forward(self, x):\n        return x + self.cv1(x) if self.add else self.cv1(x)\n\n\nclass Attention(nn.Module):\n    \"\"\"Attention module for YOLOv11\"\"\"\n    def __init__(self, dim, num_heads=8, attn_ratio=0.5):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.key_dim = self.head_dim  # Make key_dim equal to head_dim\n        self.scale = self.key_dim ** -0.5\n        nh_kd = self.key_dim * num_heads\n        h = dim + nh_kd * 2\n        self.qkv = Conv(dim, h, 1, act=False)\n        self.proj = Conv(dim, dim, 1, act=False)\n        self.pe = Conv(dim, dim, 3, 1, g=dim, act=False)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        N = H * W\n        qkv = self.qkv(x)\n        q, k, v = qkv.split([C, self.key_dim * self.num_heads, self.key_dim * self.num_heads], dim=1)\n        \n        # Reshape for multi-head attention: [B, num_heads, N, head_dim/key_dim]\n        q = q.reshape(B, self.num_heads, C // self.num_heads, N).permute(0, 1, 3, 2)\n        k = k.reshape(B, self.num_heads, self.key_dim, N).permute(0, 1, 3, 2)\n        v = v.reshape(B, self.num_heads, self.key_dim, N).permute(0, 1, 3, 2)\n        \n        # Attention: match dimensions by using key_dim for both q and k\n        attn = (q[..., :self.key_dim] @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        x = (attn @ v).permute(0, 1, 3, 2).reshape(B, C, H, W)\n        \n        x = self.proj(x) + self.pe(v.permute(0, 1, 3, 2).reshape(B, -1, H, W))\n        return x\n\n\nclass PSA(nn.Module):\n    \"\"\"Position-Sensitive Attention for YOLOv11\"\"\"\n    def __init__(self, c1, c2, e=0.5):\n        super().__init__()\n        c_ = int(c1 * e)\n        self.cv1 = Conv(c1, 2 * c_, 1)\n        self.cv2 = Conv(2 * c_, c2, 1)\n        self.attn = Attention(c_, attn_ratio=0.5, num_heads=c_ // 64)\n        self.ffn = nn.Sequential(\n            Conv(c_, c_ * 2, 1),\n            Conv(c_ * 2, c_, 1, act=False)\n        )\n\n    def forward(self, x):\n        a, b = self.cv1(x).chunk(2, 1)\n        b = b + self.attn(b)\n        b = b + self.ffn(b)\n        return self.cv2(torch.cat((a, b), 1))\n\n\n# ============================================================================\n# Detection Head\n# ============================================================================\n\ndef make_anchors(feats, strides, grid_cell_offset=0.5):\n    \"\"\"Generate anchor points and stride tensors for all feature map levels.\n\n    Args:\n        feats: list of feature map tensors (used only for their H, W, device, dtype)\n        strides: stride value for each feature level\n        grid_cell_offset: offset for grid cell centers (default 0.5)\n\n    Returns:\n        anchor_points: [total_anchors, 2] grid center coordinates\n        stride_tensor: [total_anchors, 1] stride for each anchor\n    \"\"\"\n    anchor_points, stride_tensor = [], []\n    assert feats is not None\n    dtype, device = feats[0].dtype, feats[0].device\n    for i, stride in enumerate(strides):\n        _, _, h, w = feats[i].shape\n        sx = torch.arange(end=w, device=device, dtype=dtype) + grid_cell_offset\n        sy = torch.arange(end=h, device=device, dtype=dtype) + grid_cell_offset\n        sy, sx = torch.meshgrid(sy, sx, indexing='ij')\n        anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))\n        stride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device))\n    return torch.cat(anchor_points), torch.cat(stride_tensor)\n\n\nclass DFL(nn.Module):\n    \"\"\"Distribution Focal Loss\"\"\"\n    def __init__(self, c1=16):\n        super().__init__()\n        self.c1 = c1\n        self.conv = nn.Conv2d(c1, 1, 1, bias=False)\n        self.conv.weight.data[:] = torch.arange(c1, dtype=torch.float).view(1, c1, 1, 1)\n        self.conv.requires_grad_(False)\n\n    def forward(self, x):\n        b, _, a = x.shape\n        # [B, 64, N] -> [B, 4, 16, N] -> [B, 16, 4, N] -> softmax over 16 bins -> conv -> [B, 1, 4, N] -> [B, 4, N]\n        return self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(b, 4, a)\n\n\nclass DetectionHead(nn.Module):\n    \"\"\"YOLOv11 Detection Head\"\"\"\n    def __init__(self, nc=80, ch=()):\n        super().__init__()\n        self.nc = nc  # number of classes\n        self.nl = len(ch)  # number of detection layers\n        self.reg_max = 16  # DFL channels\n        self.no = nc + self.reg_max * 4  # number of outputs per anchor\n        self.stride = torch.tensor([8, 16, 32], dtype=torch.float32)  # default strides for P3/P4/P5\n        self._stride_computed = False\n\n        c2, c3 = max((16, ch[0] // 4, self.reg_max * 4)), max(ch[0], min(self.nc, 100))\n        self.cv2 = nn.ModuleList(\n            nn.Sequential(Conv(x, c2, 3), Conv(c2, c2, 3), nn.Conv2d(c2, 4 * self.reg_max, 1)) for x in ch\n        )\n        self.cv3 = nn.ModuleList(\n            nn.Sequential(Conv(x, c3, 3), Conv(c3, c3, 3), nn.Conv2d(c3, self.nc, 1)) for x in ch\n        )\n        self.dfl = DFL(self.reg_max) if self.reg_max > 1 else nn.Identity()\n\n    def forward(self, x):\n        # Compute strides dynamically on first forward pass\n        if not self._stride_computed:\n            # x[i] has shape [B, C, H_i, W_i]; input is assumed 640\n            # stride_i = 640 / H_i (since input is square)\n            input_h = x[0].shape[2] * 8  # P3 is stride 8 from input\n            for i in range(self.nl):\n                self.stride[i] = input_h / x[i].shape[2]\n            self.stride = self.stride.to(x[0].device)\n            self._stride_computed = True\n\n        for i in range(self.nl):\n            x[i] = torch.cat((self.cv2[i](x[i]), self.cv3[i](x[i])), 1)\n\n        if self.training:\n            return x\n\n        # Inference path: decode boxes using anchor grid\n        anchors, strides = make_anchors(x, self.stride)\n        # anchors: [total_anchors, 2], strides: [total_anchors, 1]\n\n        x_cat = torch.cat([xi.view(x[0].shape[0], self.no, -1) for xi in x], 2)\n        box, cls = x_cat.split((self.reg_max * 4, self.nc), 1)\n\n        # DFL: [B, 64, N] -> [B, 4, N] (expected LTRB distances in grid units)\n        dbox = self.dfl(box)  # [B, 4, N]\n\n        # Decode LTRB to cx, cy, w, h\n        # anchors is [N, 2], transpose to [2, N] for broadcasting with [B, 4, N]\n        anc = anchors.transpose(0, 1).unsqueeze(0)  # [1, 2, N]\n        lt, rb = dbox.chunk(2, 1)  # each [B, 2, N] â€” (left, top) and (right, bottom)\n        x1y1 = anc - lt   # top-left in grid coords\n        x2y2 = anc + rb   # bottom-right in grid coords\n        cxcy = (x1y1 + x2y2) / 2  # center\n        wh = x2y2 - x1y1          # width, height\n        dbox = torch.cat([cxcy, wh], dim=1)  # [B, 4, N] as cx, cy, w, h in grid units\n\n        # Scale to pixel coordinates\n        str_t = strides.transpose(0, 1).unsqueeze(0)  # [1, 1, N]\n        dbox = dbox * str_t\n\n        y = torch.cat((dbox, cls.sigmoid()), 1)  # [B, 4+nc, N]\n        return y, x\n\n\n# ============================================================================\n# YOLOv11 Backbone and Neck\n# ============================================================================\n\nclass YOLOv11Backbone(nn.Module):\n    \"\"\"YOLOv11 Backbone\"\"\"\n    def __init__(self, channels_list, depth_list, use_psa=True):\n        super().__init__()\n        c1, c2, c3, c4, c5 = channels_list\n        d1, d2, d3, d4 = depth_list\n\n        # Stem\n        self.stem = Conv(3, c1, 3, 2)  # P1/2\n\n        # Stage 1\n        self.stage1 = nn.Sequential(\n            Conv(c1, c2, 3, 2),  # P2/4\n            C2fCIB(c2, c2, d1, True, lk=True)\n        )\n\n        # Stage 2\n        self.stage2 = nn.Sequential(\n            Conv(c2, c3, 3, 2),  # P3/8\n            C2fCIB(c3, c3, d2, True, lk=True)\n        )\n\n        # Stage 3\n        self.stage3 = nn.Sequential(\n            Conv(c3, c4, 3, 2),  # P4/16\n            C2fCIB(c4, c4, d3, True, lk=True)\n        )\n\n        # Stage 4\n        stage4_layers = [\n            Conv(c4, c5, 3, 2),  # P5/32\n            C2fCIB(c5, c5, d4, True, lk=True)\n        ]\n        if use_psa:\n            stage4_layers.append(PSA(c5, c5))\n        self.stage4 = nn.Sequential(*stage4_layers)\n\n    def forward(self, x):\n        x = self.stem(x)\n        p2 = self.stage1(x)\n        p3 = self.stage2(p2)\n        p4 = self.stage3(p3)\n        p5 = self.stage4(p4)\n        return [p3, p4, p5]\n\n\nclass YOLOv11Neck(nn.Module):\n    \"\"\"YOLOv11 Neck (FPN + PAN)\"\"\"\n    def __init__(self, channels_list, depth_list):\n        super().__init__()\n        c3, c4, c5 = channels_list[2], channels_list[3], channels_list[4]\n        d1, d2, d3 = depth_list[1:4]\n\n        # Top-down pathway\n        self.up1 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.c2f1 = C2fCIB(c5 + c4, c4, d2, shortcut=False)\n\n        self.up2 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.c2f2 = C2fCIB(c4 + c3, c3, d1, shortcut=False)\n\n        # Bottom-up pathway\n        self.down1 = Conv(c3, c3, 3, 2)\n        self.c2f3 = C2fCIB(c3 + c4, c4, d2, shortcut=False)\n\n        self.down2 = Conv(c4, c4, 3, 2)\n        self.c2f4 = C2fCIB(c4 + c5, c5, d3, shortcut=False)\n\n    def forward(self, features):\n        p3, p4, p5 = features\n        \n        # Top-down\n        x = self.up1(p5)\n        x = torch.cat([x, p4], dim=1)\n        x = self.c2f1(x)\n        p4_out = x\n        \n        x = self.up2(x)\n        x = torch.cat([x, p3], dim=1)\n        p3_out = self.c2f2(x)\n        \n        # Bottom-up\n        x = self.down1(p3_out)\n        x = torch.cat([x, p4_out], dim=1)\n        p4_out = self.c2f3(x)\n        \n        x = self.down2(p4_out)\n        x = torch.cat([x, p5], dim=1)\n        p5_out = self.c2f4(x)\n        \n        return [p3_out, p4_out, p5_out]\n\n\n# ============================================================================\n# Complete YOLOv11 Model\n# ============================================================================\n\nclass YOLOv11Model(nn.Module):\n    \"\"\"Complete YOLOv11 Model\"\"\"\n    def __init__(self, nc=80, model_size='n'):\n        super().__init__()\n        self.nc = nc\n        \n        # Model configurations: [channels, depth]\n        # channels: [c1, c2, c3, c4, c5] for stem, stage1, stage2(P3), stage3(P4), stage4(P5)\n        configs = {\n            'n': ([16, 32, 64, 128, 256], [1, 2, 2, 1]),      # nano\n            's': ([32, 64, 128, 256, 512], [1, 2, 2, 1]),     # small\n            'm': ([48, 96, 192, 384, 768], [2, 4, 4, 2]),     # medium\n            'l': ([64, 128, 256, 512, 512], [3, 6, 6, 3]),    # large\n            'x': ([80, 160, 320, 640, 640], [4, 8, 8, 4]),    # xlarge\n        }\n        \n        channels, depths = configs.get(model_size, configs['n'])\n        \n        self.backbone = YOLOv11Backbone(channels, depths)\n        self.neck = YOLOv11Neck(channels, depths)\n        \n        # Detection head\n        ch = [channels[2], channels[3], channels[4]]  # channels for P3, P4, P5\n        self.head = DetectionHead(nc, ch)\n        \n        # Initialize weights\n        self._initialize_weights()\n\n    def forward(self, x):\n        # Backbone\n        features = self.backbone(x)\n        # Neck\n        features = self.neck(features)\n        # Head\n        return self.head(features)\n\n    def _initialize_weights(self):\n        \"\"\"Initialize model weights\"\"\"\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) and m.weight.requires_grad:\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n\n# ============================================================================\n# Utility Functions\n# ============================================================================\n\ndef autopad(k, p=None, d=1):\n    \"\"\"Auto-calculate padding for 'same' shape\"\"\"\n    if d > 1:\n        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]\n    if p is None:\n        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]\n    return p\n\n\ndef make_divisible(x, divisor):\n    \"\"\"Returns nearest x divisible by divisor\"\"\"\n    if isinstance(divisor, torch.Tensor):\n        divisor = int(divisor.max())\n    return math.ceil(x / divisor) * divisor\n","metadata":{"_uuid":"7b1ecb13-3514-4e00-adf3-02075bbf9b6c","_cell_guid":"f258482e-9cc2-491e-be7c-8f8c6b8b8acb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-11T11:01:01.773716Z","iopub.execute_input":"2026-02-11T11:01:01.774097Z","iopub.status.idle":"2026-02-11T11:01:06.151492Z","shell.execute_reply.started":"2026-02-11T11:01:01.774066Z","shell.execute_reply":"2026-02-11T11:01:06.150547Z"}},"outputs":[],"execution_count":1}]}